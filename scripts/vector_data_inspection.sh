#!/bin/bash

echo "üîç COMPLETE VECTOR DATABASE DATA INSPECTION"
echo "============================================"
echo ""

# Check local directory first, then fallback to original location
LOCAL_MEMORY_DIR="./.mcp-memory"
ORIGINAL_MEMORY_DIR="/Users/kevinbrown/servers/.mcp-memory"

if [ -d "$LOCAL_MEMORY_DIR" ]; then
    MEMORY_DIR="$LOCAL_MEMORY_DIR"
else
    MEMORY_DIR="$ORIGINAL_MEMORY_DIR"
fi

# Function to check ChromaDB
check_chromadb() {
    echo "1Ô∏è‚É£ CHROMADB VECTOR DATABASE CHECK"
    echo "================================="
    
    # Check if ChromaDB is running
    if curl -s http://127.0.0.1:8000/api/v1/heartbeat >/dev/null 2>&1; then
        echo "‚úÖ ChromaDB v1 is responding"
        API_VERSION="v1"
    elif curl -s http://127.0.0.1:8000/api/v2/heartbeat >/dev/null 2>&1; then
        echo "‚úÖ ChromaDB v2 is responding"
        API_VERSION="v2"
    else
        echo "‚ùå ChromaDB server not running"
        echo "   Start with: npm run start-chromadb"
        return 1
    fi
    
    echo "Using API version: $API_VERSION"
    echo ""
    
    # Get collection data
    if [ "$API_VERSION" = "v2" ]; then
        echo "üìä Vector Collection Data:"
        echo "--------------------------"
        
        # Get all documents
        COLLECTION_DATA=$(curl -s -X POST "http://127.0.0.1:8000/api/v2/collections/llm_conversation_memory/get" \
             -H "Content-Type: application/json" \
             -d '{}')
        
        if echo "$COLLECTION_DATA" | jq . >/dev/null 2>&1; then
            # Count documents
            DOC_COUNT=$(echo "$COLLECTION_DATA" | jq '.documents | length')
            EMBEDDING_COUNT=$(echo "$COLLECTION_DATA" | jq '.embeddings | length')
            METADATA_COUNT=$(echo "$COLLECTION_DATA" | jq '.metadatas | length')
            
            echo "üìà Collection Statistics:"
            echo "   Documents: $DOC_COUNT"
            echo "   Embeddings: $EMBEDDING_COUNT"  
            echo "   Metadata entries: $METADATA_COUNT"
            echo ""
            
            if [ "$DOC_COUNT" -gt 0 ]; then
                echo "üìÑ Sample Document Content:"
                echo "$COLLECTION_DATA" | jq -r '.documents[0]' | head -3
                echo "..."
                echo ""
                
                echo "üè∑Ô∏è Sample Metadata:"
                echo "$COLLECTION_DATA" | jq '.metadatas[0]'
                echo ""
                
                echo "üî¢ Sample Embedding Info:"
                EMBEDDING_LENGTH=$(echo "$COLLECTION_DATA" | jq '.embeddings[0] | length')
                EMBEDDING_SAMPLE=$(echo "$COLLECTION_DATA" | jq '.embeddings[0][:5]')
                echo "   Dimensions: $EMBEDDING_LENGTH"
                echo "   First 5 values: $EMBEDDING_SAMPLE"
                echo ""
                
                # Calculate storage size
                DOC_SIZE=$(echo "$COLLECTION_DATA" | jq -r '.documents[0]' | wc -c)
                METADATA_SIZE=$(echo "$COLLECTION_DATA" | jq '.metadatas[0]' | wc -c)
                EMBEDDING_SIZE=$((EMBEDDING_LENGTH * 8)) # 8 bytes per float64
                TOTAL_SIZE=$((DOC_SIZE + METADATA_SIZE + EMBEDDING_SIZE))
                
                echo "üíæ Storage Size Per Entry:"
                echo "   Document text: ~$DOC_SIZE bytes"
                echo "   Metadata: ~$METADATA_SIZE bytes"
                echo "   Embedding vector: ~$EMBEDDING_SIZE bytes"
                echo "   Total per conversation: ~$TOTAL_SIZE bytes"
                echo ""
            else
                echo "üì≠ No documents found in vector database"
            fi
        else
            echo "‚ùå Failed to parse ChromaDB response"
            echo "Raw response: $COLLECTION_DATA"
        fi
    fi
}

# Function to check JSON files
check_json_files() {
    echo ""
    echo "2Ô∏è‚É£ JSON BACKUP FILES CHECK"
    echo "=========================="
    
    if [ -d "$MEMORY_DIR" ]; then
        echo "‚úÖ Memory directory found: $MEMORY_DIR"
        echo ""
        
        JSON_FILES=$(find "$MEMORY_DIR" -name "*.json" 2>/dev/null)
        if [ -n "$JSON_FILES" ]; then
            echo "üìÅ Available Session Files:"
            echo "$JSON_FILES" | while read file; do
                SESSION_NAME=$(basename "$file" .json)
                FILE_SIZE=$(stat -f%z "$file" 2>/dev/null || stat -c%s "$file" 2>/dev/null)
                RECORD_COUNT=$(jq '. | length' "$file" 2>/dev/null)
                
                echo "   üìÑ $SESSION_NAME"
                echo "      Size: $FILE_SIZE bytes"
                echo "      Conversations: $RECORD_COUNT"
            done
            echo ""
            
            # Analyze first file in detail
            FIRST_FILE=$(echo "$JSON_FILES" | head -1)
            if [ -f "$FIRST_FILE" ]; then
                echo "üîç DETAILED ANALYSIS: $(basename "$FIRST_FILE")"
                echo "================================================"
                
                # Check if file has data
                RECORD_COUNT=$(jq '. | length' "$FIRST_FILE" 2>/dev/null)
                if [ "$RECORD_COUNT" -gt 0 ]; then
                    echo "üìä Data Structure:"
                    jq -r '.[0] | keys | join(", ")' "$FIRST_FILE" 2>/dev/null
                    echo ""
                    
                    echo "üìè Content Sizes (First Record):"
                    jq -r '.[0] | 
                    "   User message: " + (.userMessage | length | tostring) + " characters" + "\n" +
                    "   Assistant response: " + (.assistantResponse | length | tostring) + " characters" + "\n" + 
                    "   Context: " + (.context // "" | tostring | length | tostring) + " characters" + "\n" +
                    "   Tags: " + (.tags // "" | tostring | length | tostring) + " characters" + "\n" +
                    "   Timestamp: " + (.timestamp | tostring)' "$FIRST_FILE" 2>/dev/null
                    echo ""
                    
                    echo "üìÑ Sample Content Preview:"
                    echo "   User Message (first 100 chars):"
                    jq -r '.[0].userMessage' "$FIRST_FILE" 2>/dev/null | head -c 100
                    echo "..."
                    echo ""
                    echo "   Assistant Response (first 100 chars):"
                    jq -r '.[0].assistantResponse' "$FIRST_FILE" 2>/dev/null | head -c 100
                    echo "..."
                    echo ""
                    
                    echo "üè∑Ô∏è Auto-Extracted Metadata:"
                    jq '.[0] | {context, tags, sessionId}' "$FIRST_FILE" 2>/dev/null
                    echo ""
                    
                    # Calculate total JSON overhead
                    RAW_CONTENT_SIZE=$(jq -r '.[0] | .userMessage + .assistantResponse' "$FIRST_FILE" | wc -c)
                    TOTAL_JSON_SIZE=$(jq '.[0]' "$FIRST_FILE" | wc -c)
                    OVERHEAD=$((TOTAL_JSON_SIZE - RAW_CONTENT_SIZE))
                    
                    echo "üíæ Storage Efficiency:"
                    echo "   Raw conversation: $RAW_CONTENT_SIZE bytes"
                    echo "   Total JSON record: $TOTAL_JSON_SIZE bytes"
                    echo "   Metadata overhead: $OVERHEAD bytes ($((OVERHEAD * 100 / TOTAL_JSON_SIZE))%)"
                    
                else
                    echo "üì≠ File exists but contains no records"
                fi
            fi
        else
            echo "üì≠ No JSON files found in memory directory"
        fi
    else
        echo "‚ùå Memory directory not found: $MEMORY_DIR"
        echo "   Memory system may not be initialized"
    fi
}

# Function to analyze what LLM actually stores
analyze_llm_behavior() {
    echo ""
    echo "3Ô∏è‚É£ LLM STORAGE BEHAVIOR ANALYSIS"
    echo "==============================="
    echo ""
    echo "üß† What gets stored when you use memory tools:"
    echo ""
    echo "‚úÖ ALWAYS STORED:"
    echo "   ‚Ä¢ Exact user message text"
    echo "   ‚Ä¢ Complete assistant response"
    echo "   ‚Ä¢ Timestamp of conversation"
    echo "   ‚Ä¢ Session ID for organization"
    echo ""
    echo "ü§ñ AUTO-GENERATED METADATA:"
    echo "   ‚Ä¢ Context keywords (extracted from content)"
    echo "   ‚Ä¢ Technical tags (docker, kubernetes, etc.)"
    echo "   ‚Ä¢ File paths and commands mentioned"
    echo "   ‚Ä¢ Project/technology classifications"
    echo ""
    echo "üö´ NOT STORED:"
    echo "   ‚Ä¢ Previous conversation history (unless explicitly referenced)"
    echo "   ‚Ä¢ System prompts or instructions"
    echo "   ‚Ä¢ File contents (only file paths/names)"
    echo "   ‚Ä¢ Real-time data or external API responses"
    echo ""
    echo "üìä TYPICAL STORAGE SIZE PER CONVERSATION:"
    echo "   ‚Ä¢ JSON backup: 2-8KB (depending on conversation length)"
    echo "   ‚Ä¢ Vector embedding: ~12KB (1536 dimensions √ó 8 bytes)"
    echo "   ‚Ä¢ Total per memory: ~15-20KB"
    echo ""
    echo "üéØ SEMANTIC SEARCH CAPABILITY:"
    echo "   ‚Ä¢ Finds similar conversations by meaning, not exact words"
    echo "   ‚Ä¢ Works across different phrasings of same concept"
    echo "   ‚Ä¢ Clusters related technical discussions"
    echo ""
}

# Run all checks
check_chromadb
check_json_files  
analyze_llm_behavior

echo ""
echo "üéØ SUMMARY: Vector Database Contents"
echo "===================================="
echo "Your memory system stores EXACTLY what you provide - no more, no less."
echo "The 'sparse' appearance might be because:"
echo "1. Only explicit memory tool calls are stored"
echo "2. Each conversation is a separate, complete record"
echo "3. Auto-generated metadata is minimal but useful"
echo "4. Vector embeddings are dense numerical representations"
echo ""
echo "To see more data, use memory tools more frequently in conversations!"
